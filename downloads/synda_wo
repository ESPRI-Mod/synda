#!/usr/local/uvcdat/2.2.0/bin/python
# -*- coding: ISO-8859-1 -*-

##################################
#  @program      synda
#  @description  Climate models data transfer program
#  @copyright    Copyright (c)2009 Centre National de la Recherche Scientifique CNRS. All Rights Reserved
#  @license      CeCILL (https://raw.githubusercontent.com/Prodiguer/synda/master/sdt/doc/LICENSE)
##################################

# Python dependencies
# pip install python-daemon==1.6.1
# pip install python-jsonrpc==0.8.4
# pip install simplejson==3.10.0
# pip install retrying

"""
Postprocessing worker

"""

import argparse
import datetime
import getpass
import grp
import imp
import logging
import os
import pwd
import signal
import ssl
import subprocess
import traceback
from argparse import RawTextHelpFormatter
from urllib2 import URLError

import daemon.pidfile
import pyjsonrpc
from pyjsonrpc.rpcerror import InternalError
from retrying import retry

if hasattr(ssl, '_create_unverified_context'):
    ssl._create_default_https_context = ssl._create_unverified_context


class NoMoreJobToProcessException(Exception):
    pass


def _get_args():
    """
    Returns parsed command line arguments.

    """
    parser = argparse.ArgumentParser(
        description="""Post-processing worker used to fork Shell child-process or load python script as module.\nThe worker deals
            with sdp database from synchro-data to input CMIP5 variable to each process.\nThe worker returns job status
            to the database with run_log.\n\nThis script contains RPC client skeleton.""",
        formatter_class=RawTextHelpFormatter)
    # positional args
    parser.add_argument(
        'action',
        nargs='?',
        default=None,
        choices=['start', 'stop', 'status'])
    # non-positional args
    parser.add_argument(
        '-d', '--debug',
        action='store_true',
        default=False,
        help='Debug mode')
    parser.add_argument(
        '-H',
        metavar='<host>',
        type=str,
        default='134.157.240.16',
        help='Remote service hostname (default is synda-dev)')
    parser.add_argument(
        '-P',
        metavar='<port>',
        type=str,
        default='18290',
        help='Remote service port')
    parser.add_argument(
        '-w',
        metavar='<password>',
        type=str,
        default='xxxxxxxx',
        help='Remote service port (default is synda-dev)')
    parser.add_argument(
        '-j',
        type=lambda s: s.split(','),
        required=False,
        metavar='<job_class>',
        help="""Only processes specified job class.\nMultiple values can be set using comma as delimiter.""")
    parser.add_argument(
        '-l',
        metavar='/var/log/synda/sdw',
        type=str,
        default='default',
        help="""Logfile directory.\nIf /var/log/synda/sdw doesn't exist /tmp is used.""")
    parser.add_argument(
        '-p',
        metavar='<pipeline>',
        type=str,
        default=None,
        help="""Only processes specified pipeline.""")
    parser.add_argument(
        '-s',
        metavar='<path>',
        type=str,
        default='/opt/synda-ipsl/synda-dev/scripts_pp',
        help="""Process script directory (default is synda-dev).""")
    parser.add_argument(
        '-t', '--test',
        action='store_true',
        default=False,
        help="""Test server connection""")
    parser.add_argument(
        '-T',
        metavar='<timeout>',
        type=int,
        default=1000,
        help="""Remote service timeout""")
    parser.add_argument(
        '-u',
        metavar='<user>',
        type=str,
        default='syndadev',
        help="""Unprivileged user""")
    parser.add_argument(
        '-g',
        metavar='<group>',
        type=str,
        default='syndadev',
        help="""Unprivileged group""")
    parser.add_argument(
        '-1', '--one-item-only',
        action='store_true',
        default=False,
        help="""Apply process on only one database entry""")
    parser.add_argument(
        '-v', '--verbose',
        action='store_true',
        help="""Verbose mode""")
    parser.add_argument(
        '-V', '--version',
        action='version',
        version="%(prog)s ({0})".format(VERSION),
        help='Program version')
    return parser.parse_args()


def init_logging(log, daemon=False, level='DEBUG'):
    """
    Initiates the logging configuration (output, date/message formatting).
    If a directory is submitted the logfile name is unique and formatted as follows:
    ``name-YYYYMMDD-HHMMSS-JOBID.log``If ``None`` the standard output is used.

    :param str log: The logfile name or directory.
    :param str level: The log level.

    """
    default = '/var/log/synda/sdw'
    if not os.path.exists(default):
        default = '/tmp'
    log_fmt = '%(asctime)s %(levelname)s %(message)s'
    log_date_fmt = '%Y/%m/%d %I:%M:%S %p'
    log_levels = {'CRITICAL': logging.CRITICAL,
                  'ERROR':    logging.ERROR,
                  'WARNING':  logging.WARNING,
                  'INFO':     logging.INFO,
                  'DEBUG':    logging.DEBUG,
                  'NOTSET':   logging.NOTSET}
    if daemon:
        logfile = 'worker.log'
    else:
        logfile = 'worker-{0}-{1}.log'.format(datetime.datetime.now().strftime("%Y%m%d-%H%M%S"), os.getpid())
    if log == 'default':
        logfile = os.path.join(default, logfile)
    else:
        if not os.path.isdir(log):
            os.makedirs(log)
        logfile = os.path.join(log, logfile)
    logging.basicConfig(filename=logfile,
                        level=log_levels[level],
                        format=log_fmt,
                        datefmt=log_date_fmt)


def get_status_output(args, **kwargs):
    """Fork process as Shell script
    Args:
        args (list): command + arguments
    Notes
        - handle exit status conversion and raise exception if child didn't complete normally
        - as 'commands' module is deprecated, use this func as replacement
        - also note that with this func, stderr and stdout are retrieved separately
          (was not the case in 'commands' module)
        - also note that there is a 'getstatusoutput' func in subprocess
          maybe better to use it directly
          (more info https://docs.python.org/3.3/library/subprocess.html#legacy-shell-invocation-functions)
    """
    kwargs['stdout'] = subprocess.PIPE
    kwargs['stderr'] = subprocess.PIPE
    kwargs['universal_newlines'] = False
    p = subprocess.Popen(args, **kwargs)
    stdout, stderr = p.communicate()
    return p.returncode, stdout, stderr


def check_user_group():
    if user is None:
        username = getpass.getuser()
    else:
        username = user


def get_unique_filename():
    return 'worker-{0}-{1}.log'.format(datetime.datetime.now().strftime("%Y%m%d-%H%M%S"), os.getpid())


def get_logfile(log_dir, filename):
    if not os.path.exists(log_dir):
        os.mkdir(log_dir)
    return os.path.join(log_dir, filename)


def run_test(service):
    try:
        buf = service.test1(1, 2)
        print "Connection test successfully completed"
    except URLError:
        logging.info('Error occur while contacting the JSONRPC server. Are JSONRPC connection parameters correctly '
                     'set (i.e. login, password, url...) ?')
        raise


def get_script_path(script_dir, job_class, extension):
    return os.path.join(script_dir, job_class + extension)


def script_exists(script_dir, job_class, extension):
    if os.path.isfile(get_script_path(script_dir, job_class, extension)):
        return True
    else:
        return False


def run_job(job, args):
    """
    Fork the process if Shell script or loads the module if Python script, both work on job dictionnary to
    communicate with the worker.

    """
    # check
    if script_exists(args.s, job['job_class'], '.py') \
            and script_exists(args.s, job['job_class'], '.sh'):
        raise Exception('Too much scripts found for this job class (job_class={0})'.format(job['job_class']))

    if script_exists(args.s, job['job_class'], '.py'):
        # run job as python module (no fork)
        task = imp.load_source(job['job_class'], get_script_path(args.s, job['job_class'], '.py'))
        try:
            task.run(job)  # warning: job gets modified here
            logging.debug('Job completes successfully')
            job['error'] = False
            job['error_msg'] = None
        except Exception, error:
            logging.debug('Exception occurs during processing: %s' % str(error))
            job['error'] = True
            job['error_msg'] = str(error)
        # if transition_return_code has not been set by the job, default value is None
        if 'transition_return_code' not in job:
            job['transition_return_code'] = None
    elif script_exists(args.s, job['job_class'], '.sh'):
        # Run job as shell script (fork)
        script_input_parameters = []
        for k, v in job['args'].iteritems():
            assert ' ' not in k
            assert ' ' not in v
            script_input_parameters.append('--{0}'.format(k))
            script_input_parameters.append(v)
        # Pass script_dir to shell script
        script_input_parameters.append('--{0}'.format('script-dir'))
        script_input_parameters.append(args.s)
        # Passe logfile to shell script
        script_input_parameters.append('--{0}'.format('worker-log'))
        script_input_parameters.append(logging.getLoggerClass().root.handlers[0].baseFilename)
        if args.verbose:
            print script_input_parameters
        (status, stdout, stderr) = get_status_output([get_script_path(args.s, job['job_class'], '.sh')] +
                                                     script_input_parameters, shell=False)
        logging.debug('Script return code: {0}'.format(status))
        # Deprecated because all script directly logs messages into worker log during running
        logging.debug('Script stdout:\n{0}'.format(stdout))
        logging.debug('Script stderr:\n{0}'.format(stderr))

        if status == 0:
            job['error'] = False
            job['error_msg'] = None
        else:
            job['error'] = True
            job['error_msg'] = None
        job['shell_script_status'] = status
        # this is always None in case of shell script
        # (i.e. shell script cannot return a transition_return_code)
        job['transition_return_code'] = None
    else:
        raise Exception('No script found for this job class (job_class={0})'.format(job['job_class']))


def process_jobs(service, args):
    try:
        logging.info('Check for waiting jobs')
        result = service.get_job(job_class=args.j, pipeline=args.p, order='fifo')
        job = result['job']
        while job and not quit:  # loop until no more job waiting
            serialized_job_attrs = ','.join(['{0}={1}'.format(k, v) for k, v in job.iteritems()])
            logging.info('Processing job ({0})'.format(serialized_job_attrs))
            if args.debug:
                print str(job)
                job['error'] = False
                job['error_msg'] = None
            else:
                # Run process on job
                run_job(job, args)  # warning: job gets modified here
            service.job_done(job)
            if args.one_item_only:
                break
            result = service.get_job(job_class=args.j, pipeline=args.p, order='fifo')
            job = result['job']
        if not quit:
            if args.one_item_only:
                if not job:
                    logging.info('No jobs to process.')
            else:
                logging.info('No more jobs to process.')
                raise NoMoreJobToProcessException()
    except NoMoreJobToProcessException:
        raise
    except InternalError, e:
        logging.error('{0}'.format(e.data))
    except Exception, e:
        logging.error('{0}'.format(e.__class__.__name__))
        logging.error('{0}'.format(str(e)))
    logging.info('Worker stopped')


@retry(wait_exponential_multiplier=30000,
       wait_exponential_max=3600000,
       retry_on_exception=lambda e: isinstance(e, NoMoreJobToProcessException))
def process_jobs_with_retry(service, args):
    """
    Retry mecanism when no more job to do use the decorator above.

    Notes
        - In daemon mode, once there is no more job to process, worker go idle,
          then periodically checks for job using the following schedule (unit=minute):
          1, 2, 4, 8, 16, 32, 60, 60, 60, 60, 60...
        - Retry forever if an NoMoreJobToProcessException occurs, raise any other errors
    """
    process_jobs(service, args)


def run(args):
    """
    Test service connection or start processing jobs

    """
    if args.verbose:
        print url
    # Create database connection service
    service = pyjsonrpc.HttpClient(url, 'sdpp', password, args.T)
    if args.test:
        run_test(service)
    else:
        process_jobs(service, args)


def stop():
    global quit
    logging.info('Worker received stop signal')
    quit = True


# daemon related funcs
def locate_sp_home():
    """
    Return Synda Post-processing location

    """
    if 'SP_HOME' in os.environ:
        return os.environ.get('SP_HOME')
    else:
        if os.path.isfile('../sdp.conf'):
            return os.path.dirname(os.getcwd())
        else:
            return None


def get_log_dir(args):
    """
    This func is used only in daemon mode

    """
    if args.l is None:
        if locate_sp_home() is not None:
            logdir = '{0}/log'.format(locate_sp_home())
        else:
            logdir = default_logdir
    else:
        logdir = args.l
    return logdir


def is_root():
    if os.geteuid() == 0:
        return True
    else:
        return False


def daemon_status():
    if is_running():
        return "Daemon running"
    else:
        return "Daemon not running"


def is_running():
    if os.path.isfile(pid_file):  # maybe this can be replaced by "pidfile.is_locked()"
        return True
    else:
        return False


def stop_daemon(pidfile):
    if is_running():
        os.kill(pidfile.read_pid(), signal.SIGTERM)
    else:
        print 'Daemon is already stopped.'


def start_daemon(args):
    global quit
    # run daemon as unprivileged user (if run as root and unprivileged user is set)
    if is_root():
        if user and group:
            unprivileged_user_mode('daemon')
    if not is_running():
        quit = False
        with context:
            try:
                # init logging
                init_logging(args.l, daemon=True)
                # must be done after the double fork
                # start job processing
                logging.info("Daemon starting ...")
                # Create database connection service
                service = pyjsonrpc.HttpClient(url, 'sdpp', password, args.T)
                process_jobs_with_retry(service, args)
                logging.info("Daemon stopped")
            except:
                traceback.print_exc(file=open(stacktrace_log_file, "a"))
    else:
        print 'Daemon is already running.'


def chown_files(files, uid, gid):
    """
    Perform chown on all files.
    Note
        'files' can contain regular file or directory.
    """
    for file_ in files:
        # this is to prevent error like "OSError: [Errno 2] No such file or directory:
        # '/var/tmp/synda/sdt/.esg/certificates'"
        if os.path.exists(file_):
            os.chown(file_, uid, gid)


def unprivileged_user_mode(mode):
    # retrieve numeric uid/gid
    uid = pwd.getpwnam(user).pw_uid
    gid = grp.getgrnam(group).gr_gid

    # be sure file permission works for unprivileged user
    li = [args.l, stacktrace_log_file, logfile]
    chown_files(li, uid, gid)

    if mode == 'daemon':
        # set daemon process identity
        context.uid = uid
        context.gid = gid
    elif mode == 'interactive':
        # set current process identity
        os.setgid(gid)
        os.setuid(uid)
    else:
        assert False


# script init.
VERSION = '{0} {1}-{2}-{3}'.format('v2.0', '2017', '03', '01')
quit = False

tmp_folder='/tmp'
pid_file = os.path.join(tmp_folder, 'sp_worker.pid')
stacktrace_log_file = os.path.join(tmp_folder, 'worker_stacktrace.log')

if __name__ == '__main__':
    args = _get_args()
    # daemon unprivileged user
    user, group, password = args.u, args.g, args.w
    check_user_group()
    url = 'https://{0}:{1}/jsonrpc'.format(args.H, args.P)
    # retrieve passwd
    if password is None:
        password = getpass.getpass()
    # retrieve script full path
    args.s = os.path.realpath(args.s)
    if args.action is None:
        # non-daemon mode
        signal.signal(signal.SIGTERM, stop)
        # run as unprivileged user (if run as root and unprivileged user is set)
        if is_root():
            if user and group:
                unprivileged_user_mode('interactive')
        # init logging
        init_logging(args.l)
        run(args)
    else:
        # daemon mode
        pidfile = daemon.pidfile.PIDLockFile(pid_file)
        context = daemon.DaemonContext(working_directory=tmp_folder, pidfile=pidfile)
        context.signal_map = {signal.SIGTERM: stop, }
        if args.action == 'start':
            start_daemon(args)
        elif args.action == 'stop':
            stop_daemon(pidfile)
        elif args.action == 'status':
            print daemon_status()
        else:
            raise Exception('Incorrect value for action')
